{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4a0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89b1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_api = KaggleApi()\n",
    "kg_api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4826c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetSavePath = Path(\"../Datasets/Source1/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8df5fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetTempPath = kagglehub.dataset_download(\"snehaanbhawal/resume-dataset\")\n",
    "# shutil.move(DatasetTempPath, DatasetSavePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3c5f1",
   "metadata": {},
   "source": [
    "# Core Functionality (Required)\n",
    "## 1. Resume Ingestion and Processing\n",
    "- Index multiple resumes: Ingest and process multiple resume files (PDF and plain text).\n",
    "- Parse content: Extract text from resumes.\n",
    "- Chunk text: Split parsed text into meaningful chunks (100–500 tokens with overlap).\n",
    "- Generate embeddings: Produce vector embeddings for each chunk using a free-tier\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1113ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = DatasetSavePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "133f45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileType(Enum):\n",
    "    NAF = 0\n",
    "    TXT = 1\n",
    "    PDF = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "686d97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_suffix_conversion = {\n",
    "    FileType.NAF: \"\",\n",
    "    FileType.TXT: \".txt\",\n",
    "    FileType.PDF: \".pdf\",\n",
    "}\n",
    "# To convert from suffix to FileType (excluding empty string):\n",
    "suffix_type_conversion = {v: k for k, v in type_suffix_conversion.items() if v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf388808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(self, file_path: Path, file_type: FileType = FileType.NAF) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.file_type: FileType = file_type\n",
    "        self.content: str = \"\"\n",
    "\n",
    "    def __getitem__(self, idx: int) -> str:\n",
    "        if idx >= len(self.content):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.content[idx]\n",
    "    \n",
    "    def read(self) -> str:\n",
    "        if self.file_type == FileType.PDF:\n",
    "            reader = PdfReader(self.file_path)\n",
    "            self.content = \"\\n\".join(page.extract_text() for page in reader.pages)\n",
    "        elif self.file_type == FileType.TXT:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                self.content = file.read()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {self.file_type}\")\n",
    "        return self.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6c0db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Files:\n",
    "    def __init__(self, file_list: list[File] = []) -> None:\n",
    "        self.file_list = []\n",
    "        for file in file_list:\n",
    "            self.file_list.append(file)\n",
    "        \n",
    "    def __getitem__(self, index: int) -> File:\n",
    "        return self.file_list[index]\n",
    "    \n",
    "    def add(self, file: File) -> None:\n",
    "        self.file_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27430bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path: Path = path\n",
    "        self.data: dict[FileType, list[File]] = {}\n",
    "    def add_file_with_type(self, file: File, file_type: FileType) -> None:\n",
    "        if file_type not in self.data:\n",
    "            self.data[file_type] = []\n",
    "        self.data[file_type].append(file)\n",
    "    def add_files_with_type(self, files: list[File], file_type: FileType) -> None:\n",
    "        if file_type not in self.data:\n",
    "            self.data[file_type] = []\n",
    "        self.data[file_type] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265a5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestCollection:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path: Path = path\n",
    "        self.collection: Collection = Collection(path)\n",
    "    \n",
    "    def _files_with_types(self, path: Path) -> dict[FileType, list[File]]:\n",
    "        files_by_type = {ftype: [] for ftype in type_suffix_conversion.keys()}\n",
    "        \n",
    "        for file in path.iterdir():\n",
    "            if file.is_file():\n",
    "                for file_type, suffix in type_suffix_conversion.items():\n",
    "                    if file.suffix.lower() == suffix:\n",
    "                        files_by_type[file_type].append(File(file, file_type))\n",
    "                        break\n",
    "        return files_by_type\n",
    "    \n",
    "    def ingest(self) -> None:\n",
    "        files_by_type = self._files_with_types(self.path)\n",
    "        for file_type, files in files_by_type.items():\n",
    "            if files:\n",
    "                self.collection.add_files_with_type(files, file_type)\n",
    "    \n",
    "    def get_collection(self) -> Collection:\n",
    "        return self.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba7dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class IngestFolders:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path: Path = path\n",
    "        self.folder_names: list[str] = []\n",
    "        self.ingesters: list[IngestCollection] = []\n",
    "    \n",
    "    def _folders_in_path(self) -> list[Path]:\n",
    "        return [folder for folder in self.path.iterdir() if folder.is_dir()]\n",
    "    \n",
    "    def ingest(self) -> None:\n",
    "        for folder in self._folders_in_path():\n",
    "            self.folder_names.append(folder.name)\n",
    "            ingester = IngestCollection(folder)\n",
    "            ingester.ingest()\n",
    "            self.ingesters.append(ingester)\n",
    "    \n",
    "    def get_collections(self) -> list[Collection]:\n",
    "        return [ingester.get_collection() for ingester in self.ingesters]\n",
    "    \n",
    "    def get_folder_names(self) -> list[str]:\n",
    "        return self.folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ingester = IngestFolders(datasetPath)\n",
    "Ingester.ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1289535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Ingester.get_collections()\n",
    "folder_names = Ingester.get_folder_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3520afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self, file: File) -> None:\n",
    "        self.file = file\n",
    "        self.reader = None\n",
    "        if file.file_type == FileType.PDF:\n",
    "            self.reader = PdfReader(file.file_path)\n",
    "    \n",
    "    def get_text(self) -> str:\n",
    "        if self.reader is None:\n",
    "            return \"\"\n",
    "        text = \"\"\n",
    "        for page in self.reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be661bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadCollection:\n",
    "    def __init__(self, collection: Collection) -> None:\n",
    "        self.collection = collection\n",
    "        self.readers: dict[FileType, list[Reader]] = {}\n",
    "        for file_type, files in collection.data.items():\n",
    "            self.readers[file_type] = [Reader(file) for file in files]\n",
    "    \n",
    "    def get_texts(self) -> dict[FileType, list[str]]:\n",
    "        texts = {}\n",
    "        for file_type, readers in self.readers.items():\n",
    "            texts[file_type] = [reader.get_text() for reader in readers]\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "507a96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadCollections:\n",
    "    def __init__(self, collections: list[Collection]) -> None:\n",
    "        self.collections = collections\n",
    "        self.read_collections: list[ReadCollection] = [ReadCollection(collection) for collection in collections]\n",
    "    \n",
    "    def get_texts(self) -> dict[FileType, list[str]]:\n",
    "        texts = {}\n",
    "        for read_collection in self.read_collections:\n",
    "            collection_texts = read_collection.get_texts()\n",
    "            for file_type, text_list in collection_texts.items():\n",
    "                if file_type not in texts:\n",
    "                    texts[file_type] = []\n",
    "                texts[file_type].extend(text_list)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b053a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_texts = ReadCollections(files).get_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49fbb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_texts_to_files(texts: dict[FileType, list[str]], output_dir: Path) -> None:\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     for file_type, text_list in texts.items():\n",
    "#         suffix = type_suffix_conversion[FileType.TXT]\n",
    "#         for idx, text in enumerate(text_list):\n",
    "#             file_name = f\"{file_type.name.lower()}_{idx}{suffix}\"\n",
    "#             with open(output_dir / file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54a59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_texts_to_files(processed_texts, Path(\"../Data/Datasets/Source2/data/processed\"))\n",
    "# print(\"Files saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2223e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_files(texts_path: Path) -> list[str]:\n",
    "    texts = []\n",
    "    for file in texts_path.glob(\"*.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe0641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "textsPath = Path(\"../Data/Datasets/Source2/data/processed\")\n",
    "texts = read_txt_files(textsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de93589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sliding_window(text: str, window_size: int = 150, step_size: int = 100) -> list[str]:\n",
    "    return [text[i:i + window_size] for i in range(0, len(text), step_size) if i + window_size <= len(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1978e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_texts(texts: list[str]):\n",
    "    window_size = 150\n",
    "    step_size = 100\n",
    "    chunked_texts = []\n",
    "    for text in texts:\n",
    "        chunks = chunk_sliding_window(text, window_size, step_size)\n",
    "        chunked_texts.append(chunks)\n",
    "    return chunked_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1eca708",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_texts_list = chunked_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24aebe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCOUNTANT\n",
      "Summary\n",
      "Financial Accountant specializing in financial planning, reporting and analysis within the Department of Defense.\n",
      "Highlights\n",
      "Accoun\n"
     ]
    }
   ],
   "source": [
    "print(chunked_texts_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6962f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "url: str = \"http://localhost:1234/v1/embeddings\"\n",
    "headers: dict[str, str] = {\"Content-Type\": \"application/json\"}\n",
    "model_name: str = \"google/gemma-3n-e4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a66f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n"
     ]
    }
   ],
   "source": [
    "payload: dict = {\"model\": model_name, \"input\": chunked_texts_list[0][0]}\n",
    "response = requests.post(\n",
    "    url=url, headers=headers, json=payload\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb9d43",
   "metadata": {},
   "source": [
    "## 2. Vector Database Integration\n",
    "- Store embeddings: Save generated embeddings in a vector database.\n",
    "- Efficient retrieval: Implement top‑K similarity search for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f14d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :D\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc76534",
   "metadata": {},
   "source": [
    "## 3. Retrieval-Augmented Generation (RAG) Chatbot\n",
    "- Job description input: Accept a text description of the job role.\n",
    "- Retrieve relevant chunks: Perform vector search to find top resume chunks.\n",
    "- LLM-based matching: Use an LLM to generate conversational answers about\n",
    "candidate fit, citing retrieved content.\n",
    "- Conversational interface: Support follow-up questions for deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb0e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :D\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d1dc4",
   "metadata": {},
   "source": [
    "# Optional Bonus Features\n",
    "## A. Web User Interface\n",
    "- Frontend (Bonus): Implement a simple web UI for file upload, job description entry, and\n",
    "chat interaction.\n",
    "- Tech Stack: Next.js with TypeScript\n",
    "- Hosting: Vercel (Hobby/Free Tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809e7167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :D\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello :D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196e4ec",
   "metadata": {},
   "source": [
    "## B. SQL-Based Metadata Search\n",
    "- Extract metadata: Tag resumes with structured metadata (skills, titles, experience).\n",
    "- Metadata storage: Save tags in a relational database (e.g., PostgreSQL alongside\n",
    "vector store).\n",
    "- Metadata API: Expose an endpoint for SQL queries (e.g., SELECT * FROM\n",
    "resume_metadata WHERE skills @> ARRAY['TypeScript'] AND\n",
    "years_experience >= 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f34a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :D\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello :D\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
